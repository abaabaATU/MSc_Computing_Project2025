{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da0a067d",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "main output: merge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770badcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5449e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_sheets(excel_path):\n",
    "    \"\"\"Load data from Excel with merged header rows, processing brain region information\"\"\"\n",
    "    xls = pd.ExcelFile(excel_path)\n",
    "    sheets_dict = {}\n",
    "    \n",
    "    for sheet_name in xls.sheet_names:\n",
    "        # Read first two rows for header processing\n",
    "        header_df = pd.read_excel(xls, sheet_name=sheet_name, nrows=2, header=None)\n",
    "        \n",
    "        # Process brain regions (first row)\n",
    "        brain_regions = []\n",
    "        current_region = None\n",
    "        for val in header_df.iloc[0, 1:].values:\n",
    "            if pd.notna(val):\n",
    "                if \"Right hemisphere ROIs\" in val:\n",
    "                    current_region = \"R\"\n",
    "                elif \"Left hemisphere ROIs\" in val:\n",
    "                    current_region = \"L\"\n",
    "                else:\n",
    "                    current_region = val  # Fallback\n",
    "            brain_regions.append(current_region)\n",
    "        \n",
    "        # Get ROI names (second row)\n",
    "        roi_names = header_df.iloc[1, 1:].values\n",
    "        \n",
    "        # Format ROI names as 'ROI (L/R)'\n",
    "        full_roi_names = []\n",
    "        for i in range(len(roi_names)):\n",
    "            roi = str(roi_names[i]).replace(\"'\", \"\")\n",
    "            region = brain_regions[i] if pd.notna(brain_regions[i]) else \"U\"  # U for unknown\n",
    "            \n",
    "            if region in [\"L\", \"R\"]:\n",
    "                full_name = f\"{roi} ({region})\"\n",
    "            else:\n",
    "                full_name = roi\n",
    "            \n",
    "            full_roi_names.append(full_name)\n",
    "        \n",
    "        # Read data (skip header rows)\n",
    "        df = pd.read_excel(xls, sheet_name=sheet_name, header=None, skiprows=2)\n",
    "        \n",
    "        # Determine condition from sheet name\n",
    "        if 'AUD' in sheet_name:\n",
    "            condition = 'Auditory'\n",
    "        elif 'VISampm' in sheet_name:\n",
    "            condition = 'Visual_anteromedialposteromedial'\n",
    "        elif 'VISarl' in sheet_name:\n",
    "            condition = 'Visual_angular_region'\n",
    "        elif 'rs2' in sheet_name:\n",
    "            condition = 'Rest'\n",
    "        else:\n",
    "            condition = 'Unknown'\n",
    "        \n",
    "        # Extract time series data\n",
    "        time = df.iloc[:, 0].values\n",
    "        signals = df.iloc[:, 1:].values.T  # Transpose to (ROIs × time)\n",
    "        \n",
    "        sheets_dict[sheet_name] = {\n",
    "            'condition': condition,\n",
    "            'time': time,\n",
    "            'signals': signals,\n",
    "            'roi_names': full_roi_names,\n",
    "            'brain_regions': brain_regions,\n",
    "            'base_roi_names': roi_names.tolist()\n",
    "        }\n",
    "    \n",
    "    return sheets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb15d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "data_root = Path(\"../data/DMD_fMRI_data_sharing\")\n",
    "timecourse_excel_path = data_root / \"Time course data (ketxyl).xlsx\"\n",
    "\n",
    "# Check if file exists\n",
    "if not timecourse_excel_path.exists():\n",
    "    print(f\"Error: Excel file not found at {timecourse_excel_path}\")\n",
    "else:\n",
    "    # Load and print data\n",
    "    print(f\"Loading Excel file: {timecourse_excel_path}\")\n",
    "    sheets_data = load_excel_sheets(timecourse_excel_path)\n",
    "    \n",
    "    # Print verification info\n",
    "    print(\"\\nLoaded sheets summary:\")\n",
    "    print(f\"Total sheets: {len(sheets_data)}\")\n",
    "    \n",
    "    # Count sheets by condition\n",
    "    condition_counts = {}\n",
    "    for sheet_data in sheets_data.values():\n",
    "        condition = sheet_data['condition']\n",
    "        condition_counts[condition] = condition_counts.get(condition, 0) + 1\n",
    "    \n",
    "    print(\"\\nSheets by condition:\")\n",
    "    for condition, count in condition_counts.items():\n",
    "        print(f\"{condition}: {count} sheet(s)\")\n",
    "    \n",
    "    # Print detailed sheet info\n",
    "    for sheet_name, stim_data in sheets_data.items():\n",
    "        print(f\"\\n\\nSheet: {sheet_name}\")\n",
    "        print(f\"Condition: {stim_data['condition']}\")\n",
    "        print(f\"Time points: {len(stim_data['time'])}\")\n",
    "        print(f\"Signal shape: {stim_data['signals'].shape} (ROIs × Time)\")\n",
    "        print(f\"First 5 ROI names: {stim_data['roi_names'][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ded8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stimulation_paradigm(file_path: str) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load stimulation paradigm from Excel and organize by filename.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to Excel file with structure:\n",
    "        - First 2 rows: headers\n",
    "        - Column A: Filename\n",
    "        - Row 2, columns E onward: ROI names\n",
    "        - Rows 3+: stimulation times\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict[str, Dict[str, Any]] where each key is a filename and value contains:\n",
    "        {\n",
    "            'roi_stim_times': Dict[str, List[float]],  # ROI to stimulation times\n",
    "            'timeline': List[Tuple[float, float, str]]  # Sorted (start, end, state)\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Read header to get ROI names (second row, columns E onward)\n",
    "    header_df = pd.read_excel(file_path, header=None, nrows=2)\n",
    "    stim_rois = header_df.iloc[1, 4:].dropna().astype(str).tolist()\n",
    "    \n",
    "    # Read data starting from row 3 (0-based index 2)\n",
    "    data_df = pd.read_excel(file_path, header=None, skiprows=2)\n",
    "    \n",
    "    # Initialize output structure\n",
    "    paradigm_data = {}\n",
    "    \n",
    "    # Process each row (each experimental run)\n",
    "    for _, row in data_df.iterrows():\n",
    "        filename = str(row[0]) if not pd.isna(row[0]) else None\n",
    "        if not filename:\n",
    "            continue\n",
    "            \n",
    "        # Initialize data for this file\n",
    "        file_data = {\n",
    "            'roi_stim_times': {roi: [] for roi in stim_rois},\n",
    "            'timeline': []\n",
    "        }\n",
    "        \n",
    "        # Collect all stimulation events for this file\n",
    "        all_events = []\n",
    "        \n",
    "        for col_idx, roi in enumerate(stim_rois, start=4):\n",
    "            if col_idx >= len(row):\n",
    "                continue\n",
    "                \n",
    "            cell_value = row[col_idx]\n",
    "            if pd.isna(cell_value):\n",
    "                continue\n",
    "                \n",
    "            # Parse time values\n",
    "            times = []\n",
    "            for t in str(cell_value).split(','):\n",
    "                try:\n",
    "                    time = float(t.strip())\n",
    "                    if time >= 0:  # Only accept non-negative times\n",
    "                        times.append(time)\n",
    "                        all_events.append((time, roi))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            \n",
    "            file_data['roi_stim_times'][roi] = times\n",
    "        \n",
    "        # Sort events by time for this file\n",
    "        all_events.sort(key=lambda x: x[0])\n",
    "        \n",
    "        # Build timeline\n",
    "        if all_events:\n",
    "            # Add initial rest period (0 to first event)\n",
    "            file_data['timeline'].append((0.0, all_events[0][0], \"Rest\"))\n",
    "            \n",
    "            # Add stimulation periods\n",
    "            for i in range(len(all_events)):\n",
    "                current_time, current_roi = all_events[i]\n",
    "                \n",
    "                # Determine end time\n",
    "                end_time = current_time + 60 # 10s stimulation + 50s recovery\n",
    "                \n",
    "                file_data['timeline'].append((current_time, end_time, current_roi))\n",
    "        else:\n",
    "            # Entire period is rest if no events\n",
    "            file_data['timeline'].append((0.0, 1.0, \"Rest\"))\n",
    "        \n",
    "        # Store in output dictionary\n",
    "        paradigm_data[filename] = file_data\n",
    "    \n",
    "    return paradigm_data, stim_rois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06a80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimpara_excel_path = data_root / \"Stimulation_paradigm.xlsx\"\n",
    "stim_data, stim_rois = load_stimulation_paradigm(stimpara_excel_path)\n",
    "\n",
    "# Print ROI names from stimulation paradigm\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ROI Names from Stimulation Paradigm\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nTotal ROIs: {len(stim_rois)}\")\n",
    "print(f\"ROI names: {stim_rois}\")\n",
    "\n",
    "# Analyze stimulation data structure\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Stimulation Data Structure Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if not stim_data:\n",
    "    print(\"Warning: stim_data is empty!\")\n",
    "else:\n",
    "    # Print first and last entry keys\n",
    "    first_key = next(iter(stim_data))\n",
    "    last_key = next(reversed(stim_data))\n",
    "    \n",
    "    print(f\"\\nTotal entries: {len(stim_data)}\")\n",
    "    print(f\"First entry key: '{first_key}'\")\n",
    "    print(f\"Last entry key: '{last_key}'\")\n",
    "    \n",
    "    # Print details of first entry\n",
    "    print(\"\\nFirst entry details:\")\n",
    "    first_entry = stim_data[first_key]\n",
    "    print(f\"Entry type: {type(first_entry)}\")\n",
    "    \n",
    "    if isinstance(first_entry, dict):\n",
    "        print(\"Available keys:\", first_entry.keys())\n",
    "        \n",
    "        # Print timeline sample\n",
    "        if 'timeline' in first_entry:\n",
    "            print(\"\\nTimeline sample (first 3 events):\")\n",
    "            for start, end, state in first_entry['timeline'][:3]:\n",
    "                print(f\"  {start:.1f}s - {end:.1f}s: {state}\")\n",
    "        \n",
    "        # Print ROI stimulation times\n",
    "        if 'roi_stim_times' in first_entry:\n",
    "            print(\"\\nROI Stimulation Times:\")\n",
    "            for roi, times in list(first_entry['roi_stim_times'].items())[:5]:  # First 5 ROIs only\n",
    "                times_sample = times[:3] if isinstance(times, (list, np.ndarray)) else [times]\n",
    "                print(f\"  {roi}: {times_sample}... (total {len(times) if hasattr(times, '__len__') else 1} events)\")\n",
    "            \n",
    "            # Check for unmatched ROIs\n",
    "            unmatched_rois = set(stim_rois) - set(first_entry['roi_stim_times'].keys())\n",
    "            if unmatched_rois:\n",
    "                print(f\"\\nWarning: {len(unmatched_rois)} ROIs in roi_names not found in stim_data:\")\n",
    "                print(list(unmatched_rois)[:5], \"...\")\n",
    "    else:\n",
    "        print(f\"Unexpected entry type. Content: {first_entry}\")\n",
    "\n",
    "# Data consistency check\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Data Consistency Check\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if stim_data and stim_rois:\n",
    "    # Check ROI consistency across all entries\n",
    "    all_rois_in_stim = set()\n",
    "    for entry in stim_data.values():\n",
    "        if isinstance(entry, dict) and 'roi_stim_times' in entry:\n",
    "            all_rois_in_stim.update(entry['roi_stim_times'].keys())\n",
    "    \n",
    "    print(f\"\\nROIs in stim_data: {len(all_rois_in_stim)}\")\n",
    "    print(f\"ROIs in roi_names: {len(stim_rois)}\")\n",
    "    \n",
    "    missing_in_stim = set(stim_rois) - all_rois_in_stim\n",
    "    extra_in_stim = all_rois_in_stim - set(stim_rois)\n",
    "    \n",
    "    if missing_in_stim:\n",
    "        print(f\"\\nWarning: {len(missing_in_stim)} ROIs in roi_names missing from stim_data:\")\n",
    "        print(list(missing_in_stim)[:5], \"...\")\n",
    "    \n",
    "    if extra_in_stim:\n",
    "        print(f\"\\nWarning: {len(extra_in_stim)} ROIs in stim_data not in roi_names:\")\n",
    "        print(list(extra_in_stim)[:5], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef58f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_up_and_merge_stimulation(\n",
    "    sheets_data: Dict[str, Dict[str, Any]],\n",
    "    stim_data: Dict[str, Dict[str, Any]],\n",
    "    sampling_rate: float = 1.0\n",
    ") -> Dict[str, Dict[str, Any]]: \n",
    "    \"\"\"\n",
    "    Align stimulation paradigm timeline with time series data and merge by experimental condition\n",
    "    \n",
    "    Args:\n",
    "        sheets_data: Time series data {sheet_name: {'signals': np.ndarray, ...}}\n",
    "        stim_data: Stimulation paradigm data {filename: {'timeline': [(start, end, state)], ...}}\n",
    "        sampling_rate: Sampling rate in Hz\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            'roi_names': List[str],  # Shared across all conditions\n",
    "            'Rest': {\n",
    "                'data': np.ndarray,  # shape: (ROIs, time_points, trials)\n",
    "                'sources': List[str]  \n",
    "            },\n",
    "            'Condition1': {\n",
    "                'data': np.ndarray,  # shape: (ROIs, time_points, trials)\n",
    "                'stim_roi': str,     # Stimulated ROI name\n",
    "                'sources': List[str] \n",
    "            },\n",
    "            ...\n",
    "        }\n",
    "    \"\"\"\n",
    "    merged_data = defaultdict(lambda: {'data': [], 'sources': []}) \n",
    "    roi_names = next(iter(sheets_data.values()))['roi_names']\n",
    "    \n",
    "    # Process each experimental file\n",
    "    for filename, file_stim_data in stim_data.items():\n",
    "        if filename not in sheets_data:\n",
    "            continue\n",
    "            \n",
    "        sheet_data = sheets_data[filename]\n",
    "        timecourse_data = sheet_data['signals']  # (ROIs, time_points)\n",
    "        \n",
    "        for start_time, end_time, state in file_stim_data['timeline']:\n",
    "            start_idx = int(start_time * sampling_rate)\n",
    "            end_idx = int(end_time * sampling_rate)\n",
    "            \n",
    "            if start_idx >= timecourse_data.shape[1]:\n",
    "                continue\n",
    "                \n",
    "            end_idx = min(end_idx, timecourse_data.shape[1])\n",
    "            segment = timecourse_data[:, start_idx:end_idx]\n",
    "            \n",
    "            merged_data[state]['data'].append(segment)\n",
    "            merged_data[state]['sources'].append(filename[2])\n",
    "    \n",
    "    # Convert to unified structure with shared roi_names\n",
    "    final_data = {'roi_names': roi_names}\n",
    "    for state, state_data in merged_data.items():\n",
    "        if state_data['data']: \n",
    "            state_entry = {\n",
    "                'data': np.stack(state_data['data'], axis=-1),\n",
    "                'sources': state_data['sources'] \n",
    "            }\n",
    "            \n",
    "            if state != \"Rest\":\n",
    "                state_entry['stim_roi'] = state\n",
    "                \n",
    "            final_data[state] = state_entry\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fbef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = line_up_and_merge_stimulation(\n",
    "    sheets_data=sheets_data,\n",
    "    stim_data=stim_data,\n",
    "    sampling_rate=0.5\n",
    ")\n",
    "\n",
    "# Inspect the results\n",
    "print(\"\\nMerged Data Structure:\")\n",
    "for condition, data in merged_data.items():\n",
    "    if condition == 'roi_names':\n",
    "        roi_names = data\n",
    "    else:\n",
    "        print(f\"\\nCondition: {condition}\")\n",
    "        print(f\"Data shape: {data['data'].shape} (ROIs×time×trials)\")\n",
    "        print(f\"Source: {data['sources']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9c9ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export merged_data\n",
    "import pickle\n",
    "\n",
    "with open('merged_data.pkl', 'wb') as f:\n",
    "    pickle.dump(merged_data, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
